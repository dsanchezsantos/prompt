import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- Configura√ß√£o da API Google Gemini ---
# Pega a chave da API do arquivo .env
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("Chave de API do Google Gemini n√£o encontrada! Crie um arquivo .env com GOOGLE_API_KEY='SUA_CHAVE'.")
    st.stop() # Interrompe a execu√ß√£o se a chave n√£o for encontrada

genai.configure(api_key=GOOGLE_API_KEY)

# Inicializa o modelo Gemini Pro
model = genai.GenerativeModel('gemini-2.5-flash')

# --- SEU CONTEXTO/PROMPT DE SISTEMA ---
# Este √© o "treinamento" que voc√™ quer que o modelo siga.
# Voc√™ pode editar este texto para definir o papel da sua IA.
system_prompt = """
A partir de agora, voc√™ √© um guia tur√≠stico extremamente entusiasmado e apaixonado por Arraial do Cabo, Rio de Janeiro.
Sua miss√£o √© dar dicas incr√≠veis de praias, passeios de barco, atividades aqu√°ticas (como mergulho e snorkeling), e pontos tur√≠sticos escondidos da regi√£o.
Sua linguagem deve ser sempre informal, amig√°vel e acolhedora, como se estivesse conversando com um amigo que vai visitar a cidade.
Use emojis ocasionalmente para transmitir seu entusiasmo! üéâ
**Regras importantes:**
1. Nunca mencione pre√ßos de passeios, hospedagens ou qualquer custo financeiro.
2. Mantenha o foco exclusivamente em Arraial do Cabo. Se a pergunta for sobre outra cidade ou assunto, responda que seu conhecimento √© exclusivo de Arraial.
3. Incentive sempre a explora√ß√£o das belezas naturais e o contato com a natureza.
4. Ao final de cada resposta, sempre pergunte se o usu√°rio tem mais alguma d√∫vida ou curiosidade sobre Arraial.
"""

st.set_page_config(page_title="Guia de Arraial do Cabo!", page_icon="üèñÔ∏è")
st.title("Bem-vindo ao seu Guia de Arraial do Cabo! üèñÔ∏è")
st.markdown("Pergunte-me qualquer coisa sobre as maravilhas desta cidade paradis√≠aca!")

# --- L√≥gica do Chatbot ---
# Inicializa o hist√≥rico de mensagens na sess√£o do Streamlit
# O primeiro item √© o prompt de sistema, que n√£o ser√° exibido, mas √© enviado √† API.
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "user", "parts": [system_prompt]} # 'user' √© o papel que define o modelo
    ]

# Exibe o hist√≥rico de mensagens (exceto o prompt de sistema inicial)
# O modelo Gemini usa 'user' para o usu√°rio e 'model' para as respostas da IA.
for message in st.session_state.messages:
    if message["parts"][0] != system_prompt: # N√£o exibe o prompt inicial
        with st.chat_message("user" if message["role"] == "user" else "assistant"):
            st.markdown(message["parts"][0])

# Campo de entrada para o usu√°rio
if prompt := st.chat_input("Qual sua d√∫vida sobre Arraial do Cabo?"):
    # Adiciona a pergunta do usu√°rio ao hist√≥rico da sess√£o (para exibir e enviar ao modelo)
    st.session_state.messages.append({"role": "user", "parts": [prompt]})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Buscando as melhores dicas de Arraial..."):
            try:
                # Cria a conversa com o modelo, incluindo o hist√≥rico e o prompt de sistema
                # O par√¢metro 'history' garante que o contexto seja mantido.
                chat = model.start_chat(history=st.session_state.messages)
                response = chat.send_message(prompt) # Envia a √∫ltima pergunta do usu√°rio

                full_response = response.text
                st.markdown(full_response)
                # Adiciona a resposta da IA ao hist√≥rico da sess√£o
                st.session_state.messages.append({"role": "model", "parts": [full_response]})
            except Exception as e:
                st.error(f"Ocorreu um erro ao processar sua pergunta: {e}")
                st.markdown("Por favor, tente novamente mais tarde.")